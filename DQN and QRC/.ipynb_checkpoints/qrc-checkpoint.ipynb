{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401f3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from tbu_gym.tbu_discrete import TruckBackerEnv_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ac72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea22eba6-221d-4279-8dec-6d2c1c7db3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Simple MLP Q / h networks\n",
    "# -------------------------\n",
    "def make_mlp(input_dim: int, output_dim: int, hidden_sizes=(128, 128), activation=nn.ReLU):\n",
    "    layers = []\n",
    "    prev = input_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.append(nn.Linear(prev, h))\n",
    "        layers.append(activation())\n",
    "        prev = h\n",
    "    layers.append(nn.Linear(prev, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers for param-traces\n",
    "# -------------------------\n",
    "def zeros_like_params(params: List[torch.nn.Parameter]):\n",
    "    return [torch.zeros_like(p.data) for p in params]\n",
    "\n",
    "def add_param_lists(a: List[torch.Tensor], b: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "def scale_param_list(a: List[torch.Tensor], scalar: float) -> List[torch.Tensor]:\n",
    "    return [scalar * x for x in a]\n",
    "\n",
    "def copy_params(params: List[torch.nn.Parameter]) -> List[torch.Tensor]:\n",
    "    return [p.data.clone() for p in params]\n",
    "\n",
    "# -------------------------\n",
    "# QRC Agent\n",
    "# -------------------------\n",
    "class QRCAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        q_lr=1e-4,\n",
    "        h_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        lamda=0.9,\n",
    "        reg_coeff=1.0,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        buffer_size=100000,\n",
    "        batch_size=64,\n",
    "        device=None,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "        # Networks\n",
    "        self.q_net = make_mlp(state_dim, action_dim).to(self.device)\n",
    "        self.h_net = make_mlp(state_dim, action_dim).to(self.device)\n",
    "\n",
    "        # Optionally: target network for Q (you can use it the same way as DQN)\n",
    "        self.target_q = make_mlp(state_dim, action_dim).to(self.device)\n",
    "        self.target_q.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # Optimizers\n",
    "        self.q_opt = optim.Adam(self.q_net.parameters(), lr=q_lr)\n",
    "        self.h_opt = optim.Adam(self.h_net.parameters(), lr=h_lr)\n",
    "\n",
    "        # Replay\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Hyperparams\n",
    "        self.gamma = gamma\n",
    "        self.lamda = lamda\n",
    "        self.reg_coeff = reg_coeff\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # Traces (initialized to zero)\n",
    "        self.q_param_list = list(self.q_net.parameters())\n",
    "        self.h_param_list = list(self.h_net.parameters())\n",
    "        self.zero_q_trace = zeros_like_params(self.q_param_list)\n",
    "        self.zero_h_grad_trace = zeros_like_params(self.h_param_list)\n",
    "        self.zero_q_grad_trace = zeros_like_params(self.q_param_list)\n",
    "\n",
    "        # Per-episode/stateful traces\n",
    "        self.h_trace_scalar = 0.0  # scalar trace for h (small z_t)\n",
    "        self.grad_h_trace = copy_params(self.h_param_list)  # z_t^{theta}\n",
    "        self.grad_q_trace = copy_params(self.q_param_list)  # z_t^{w}\n",
    "\n",
    "    # ---------- interaction ----------\n",
    "    def agent_policy(self, state: np.ndarray) -> int:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        s = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.q_net(s)\n",
    "        return int(qvals.argmax().item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # ---------- training helpers ----------\n",
    "    def _zero_grad_params(self, model: nn.Module):\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "    def _params_to_list(self, params: List[torch.nn.Parameter]) -> List[torch.Tensor]:\n",
    "        return [p for p in params]\n",
    "\n",
    "    def _get_grads_of_scalar_wrt_params(self, scalar: torch.Tensor, params: List[torch.nn.Parameter]):\n",
    "        \"\"\"Return list of gradients (same order as params) for scalar w.r.t. params.\n",
    "           Uses create_graph=False by default, but keep graph when needed externally by caller.\n",
    "        \"\"\"\n",
    "        grads = torch.autograd.grad(scalar, params, retain_graph=True, allow_unused=True)\n",
    "        grads = [g if (g is not None) else torch.zeros_like(p.detach()) for g, p in zip(grads, params)]\n",
    "        return grads\n",
    "\n",
    "    # ---------- trace update ----------\n",
    "    def _update_traces(self, rho: float, h_val: float, q_grads, h_grads):\n",
    "        # scalar trace for h\n",
    "        self.h_trace_scalar = rho * self.gamma * self.lamda * self.h_trace_scalar + float(h_val)\n",
    "\n",
    "        # grad traces: grad_h_trace <- rho*gamma*lambda*grad_h_trace + grad_h\n",
    "        self.grad_h_trace = [\n",
    "            rho * self.gamma * self.lamda * old + new.detach()\n",
    "            for old, new in zip(self.grad_h_trace, h_grads)\n",
    "        ]\n",
    "        # grad_q_trace <- rho*gamma*lambda*grad_q_trace + grad_q\n",
    "        self.grad_q_trace = [\n",
    "            rho * self.gamma * self.lamda * old + new.detach()\n",
    "            for old, new in zip(self.grad_q_trace, q_grads)\n",
    "        ]\n",
    "\n",
    "    def _reset_traces(self):\n",
    "        self.h_trace_scalar = 0.0\n",
    "        self.grad_h_trace = copy_params(self.h_param_list)\n",
    "        self.grad_q_trace = copy_params(self.q_param_list)\n",
    "\n",
    "    # ---------- main training step using a batch ----------\n",
    "    def train_with_mem(self, use_target_for_next=True):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # tensors\n",
    "        states_t = torch.FloatTensor(np.stack(states)).to(self.device)         # (B, S)\n",
    "        next_states_t = torch.FloatTensor(np.stack(next_states)).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)                 # (B,)\n",
    "        rewards_t = torch.FloatTensor(rewards).to(self.device)\n",
    "        dones_t = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        B = len(batch)\n",
    "\n",
    "        # We'll accumulate parameter-grad updates across the batch before applying\n",
    "        q_param_shapes = [p.data.shape for p in self.q_param_list]\n",
    "        h_param_shapes = [p.data.shape for p in self.h_param_list]\n",
    "\n",
    "        # initialize accumulated updates (same structure as params)\n",
    "        acc_q_grads = [torch.zeros_like(p.data) for p in self.q_param_list]\n",
    "        acc_h_grads = [torch.zeros_like(p.data) for p in self.h_param_list]\n",
    "\n",
    "        # We'll also accumulate whether to reset traces (for done samples).\n",
    "        # For simplicity: we treat each sample independently for rho (here we use 1 if greedy, else 0).\n",
    "        # Use greedy on-policy assumption: rho=1 (best-case). If you want importance weights, compute them here.\n",
    "        for i in range(B):\n",
    "            s = states_t[i:i+1]       # shape (1, S)\n",
    "            a = int(actions_t[i].item())\n",
    "            ns = next_states_t[i:i+1]\n",
    "            r = rewards_t[i].unsqueeze(0)\n",
    "            done = bool(dones_t[i].item())\n",
    "\n",
    "            # Q(s,a)\n",
    "            q_vals = self.q_net(s)\n",
    "            q_s_a = q_vals[0, a]\n",
    "\n",
    "            # next state value (max_a' Q(next))\n",
    "            if use_target_for_next:\n",
    "                with torch.no_grad():\n",
    "                    next_q_vals = self.target_q(ns)\n",
    "                    max_next = next_q_vals.max(1)[0]  # shape (1,)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    next_q_vals = self.q_net(ns)\n",
    "                    max_next = next_q_vals.max(1)[0]\n",
    "\n",
    "            td_target = r + (1.0 - float(done)) * self.gamma * max_next  # shape (1,)\n",
    "            td_error = (td_target - q_s_a)  # scalar tensor (1,)\n",
    "\n",
    "            # grads of td_error w.r.t q params\n",
    "            q_params = list(self.q_net.parameters())\n",
    "            # Ensure that q_s_a uses the graph for param derivatives\n",
    "            td_scalar = td_error.squeeze()\n",
    "            q_grads = self._get_grads_of_scalar_wrt_params(td_scalar, q_params)  # list of grads\n",
    "\n",
    "            # h(s,a) and grads wrt h params\n",
    "            h_vals = self.h_net(s)\n",
    "            h_s_a = h_vals[0, a]\n",
    "            h_params = list(self.h_net.parameters())\n",
    "            h_grads = self._get_grads_of_scalar_wrt_params(h_s_a.squeeze(), h_params)\n",
    "\n",
    "            # rho: here we use 1.0 for greedy policy; you can compute importance sampling ratio if using off-policy.\n",
    "            rho = 1.0\n",
    "\n",
    "            # update traces for this sample (we treat per-sample trace update and then use them directly)\n",
    "            # Note: we do not maintain per-sample traces in replay â€” the canonical approach uses online traces.\n",
    "            # For replay, the usual practical approximation is to treat rho=1 for samples sampled from replay;\n",
    "            # this code follows simpler approach: accumulate gradient-traces per sample local to the update.\n",
    "            # Here we simulate trace values for the sample:\n",
    "            h_trace_sample = rho * self.gamma * self.lamda * 0.0 + float(h_s_a.detach().cpu().numpy())\n",
    "\n",
    "            # compute q update parts based on the paper:\n",
    "            # q_update = - h_trace * grad_td_error  (GTD2 base)\n",
    "            # if gradient_correction (we include it always here):\n",
    "            #   + td_error * grad_q_trace  - h * grad_q\n",
    "            # Implementation detail: grad_q_trace is the stored trace across previous updates; for replay we approximate with stored self.grad_q_trace\n",
    "            # We'll use stored grad traces (self.grad_q_trace / self.grad_h_trace) as the trace memory.\n",
    "            q_update_per_param = []\n",
    "            for idx, g_td in enumerate(q_grads):\n",
    "                # g_td may be small shaped; ensure shape matches q param\n",
    "                base = - (self.h_trace_scalar + h_trace_sample) * g_td.detach()  # -h_trace * grad_td\n",
    "                # gradient correction: td_error * grad_q_trace - h * grad_q\n",
    "                corr = td_scalar.detach() * self.grad_q_trace[idx] - h_s_a.detach() * g_td.detach()\n",
    "                total = base + corr\n",
    "                q_update_per_param.append(total)\n",
    "\n",
    "            # accumulate into acc_q_grads\n",
    "            for idx in range(len(acc_q_grads)):\n",
    "                acc_q_grads[idx] += q_update_per_param[idx].detach()\n",
    "\n",
    "            # update h: h_update = -( delta_z_h + h_h_grad + beta_params )\n",
    "            # delta_z_h = td_error * grad_h_trace (use stored grad_h_trace as approximation)\n",
    "            # h_h_grad = -h * h_grads  (in jax version they used -h * grad_h)\n",
    "            # beta_params = -reg_coeff * h_params\n",
    "            h_update_per_param = []\n",
    "            for idx, hg in enumerate(h_grads):\n",
    "                delta_z_h = td_scalar.detach() * self.grad_h_trace[idx]\n",
    "                h_h_grad = - h_s_a.detach() * hg.detach()\n",
    "                beta_param = - self.reg_coeff * h_params[idx].data\n",
    "                # In the jax code they combined with a negative piece; we produce gradient to apply directly:\n",
    "                # final gradient (to ascend the objective, but in pytorch we'll set this as grad to do descent)\n",
    "                # The jax code does: h_update = -( delta_z_h + h_h_grad + beta_params ). We'll follow that:\n",
    "                h_update = - (delta_z_h + h_h_grad + beta_param)\n",
    "                h_update_per_param.append(h_update)\n",
    "\n",
    "            # accumulate h grads\n",
    "            for idx in range(len(acc_h_grads)):\n",
    "                acc_h_grads[idx] += h_update_per_param[idx].detach()\n",
    "\n",
    "            # For trace statefulness: reset traces if done (approx)\n",
    "            if done:\n",
    "                # reset stored traces\n",
    "                self._reset_traces()\n",
    "            else:\n",
    "                # update stored traces globally (we use the q_grads and h_grads computed above)\n",
    "                # This is a practical approximation for the full online algorithm.\n",
    "                self.h_trace_scalar = rho * self.gamma * self.lamda * self.h_trace_scalar + float(h_s_a.detach())\n",
    "                self.grad_h_trace = [rho * self.gamma * self.lamda * old + new.detach()\n",
    "                                     for old, new in zip(self.grad_h_trace, h_grads)]\n",
    "                self.grad_q_trace = [rho * self.gamma * self.lamda * old + new.detach()\n",
    "                                     for old, new in zip(self.grad_q_trace, q_grads)]\n",
    "\n",
    "        # Average accumulated grads across batch\n",
    "        acc_q_grads = [g / float(B) for g in acc_q_grads]\n",
    "        acc_h_grads = [g / float(B) for g in acc_h_grads]\n",
    "\n",
    "        # Apply gradients to q_net (note: optimizer expects .grad on parameters)\n",
    "        self._zero_grad_params(self.q_net)\n",
    "        for p, g in zip(self.q_net.parameters(), acc_q_grads):\n",
    "            # Flax multiplies by -1 in their update pipeline; here we apply gradient descent so set grad = g\n",
    "            p.grad = g.to(self.device)\n",
    "        self.q_opt.step()\n",
    "\n",
    "        # Apply gradients to h_net\n",
    "        self._zero_grad_params(self.h_net)\n",
    "        for p, g in zip(self.h_net.parameters(), acc_h_grads):\n",
    "            p.grad = g.to(self.device)\n",
    "        self.h_opt.step()\n",
    "\n",
    "        # epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # ---------- utility ----------\n",
    "    def update_target(self):\n",
    "        self.target_q.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def save(self, path_prefix):\n",
    "        torch.save(self.q_net.state_dict(), path_prefix + \"_q.pth\")\n",
    "        torch.save(self.h_net.state_dict(), path_prefix + \"_h.pth\")\n",
    "\n",
    "    def load(self, path_prefix):\n",
    "        self.q_net.load_state_dict(torch.load(path_prefix + \"_q.pth\", map_location=self.device))\n",
    "        self.h_net.load_state_dict(torch.load(path_prefix + \"_h.pth\", map_location=self.device))\n",
    "        self.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37620abb-5217-4436-9ecb-176dfc299d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d34881",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qrc_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtbu_gym\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtbu_discrete\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TruckBackerEnv_D\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqrc_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QRC_Agent  \u001b[38;5;66;03m# import the agent we defined\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Hyperparameters\u001b[39;00m\n\u001b[32m      7\u001b[39m num_episodes = \u001b[32m1000\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'qrc_agent'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tbu_gym.tbu_discrete import TruckBackerEnv_D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 500\n",
    "gamma = 0.99\n",
    "q_lr = 1e-4\n",
    "h_lr = 1e-3\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay = 0.99997\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "target_update_freq = 5  # optional, can sync Q target if you want\n",
    "\n",
    "# Environment setup\n",
    "env = TruckBackerEnv_D(render_mode=None)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize QRC agent\n",
    "agent = QRCAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    q_lr=q_lr,\n",
    "    h_lr=h_lr,\n",
    "    gamma=gamma,\n",
    "    lamda=0.9,\n",
    "    reg_coeff=1.0,\n",
    "    epsilon=epsilon_start,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    epsilon_min=epsilon_min,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Storage for plotting\n",
    "episode_rewards = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        # Epsilon-greedy action\n",
    "        action = agent.agent_policy(np.array(state, dtype=np.float32))\n",
    "\n",
    "        # Step environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store transition\n",
    "        agent.remember(state, action, reward, next_state, float(done))\n",
    "\n",
    "        # Train agent with memory\n",
    "        agent.train_with_mem()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Optional: sync target network periodically (similar to DQN)\n",
    "    if episode % target_update_freq == 0:\n",
    "        agent.update_target()\n",
    "\n",
    "    # Track rewards\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('QRC Training on TruckBackerEnv_D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1bfea-0723-4678-8591-5eac57d8b7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
