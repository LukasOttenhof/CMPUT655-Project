{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "401f3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from tbu_gym.tbu_discrete import TruckBackerEnv_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ac72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea22eba6-221d-4279-8dec-6d2c1c7db3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QRC(λ)\n",
    "from functools import partial\n",
    "from typing import NamedTuple\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from jax.tree_util import tree_map\n",
    "from gtd_algos.src import tree\n",
    "from gtd_algos.src.algorithms.agent import Agent\n",
    "from gtd_algos.src.configs.Config import Config\n",
    "from gtd_algos.src.agents.value_networks import DenseQNetwork, MinAtarQNetwork\n",
    "from gtd_algos.src.nets.MLP import sparse_init\n",
    "from gtd_algos.src.algorithms.streamq import StreamQAgent\n",
    "\n",
    "class AgentState(NamedTuple):\n",
    "    agent_config: Config\n",
    "    train_state: TrainState\n",
    "    h_state: TrainState\n",
    "    h_trace: jnp.ndarray      # the scalar trace for h (small z_t from the paper)\n",
    "    grad_h_trace: jnp.ndarray # the trace of the gradient of h (z_t^{theta} from the paper)\n",
    "    grad_q_trace: jnp.ndarray # the trace of the gradient of v (z_t^{w} from the paper)\n",
    "\n",
    "\n",
    "def init_agent_state(agent_config: Config, action_dim: int, obs_shape: tuple, rng: jax.random.PRNGKey):\n",
    "    net_kwargs = {\n",
    "        'action_dim': action_dim,\n",
    "        'layer_norm': agent_config.layer_norm,\n",
    "        'activation': agent_config.activation,\n",
    "        'kernel_init': sparse_init(sparsity=agent_config.sparse_init),\n",
    "    }\n",
    "    net_arch = agent_config.net_arch\n",
    "    if net_arch == 'mlp':\n",
    "        net_kwargs['hiddens'] = agent_config.mlp_layers\n",
    "    \n",
    "    \n",
    "    train_states = []\n",
    "    lrs = [agent_config.q_lr, agent_config.h_lr_scale*agent_config.q_lr]\n",
    "    # one network for q and one for h\n",
    "    for net in range(2):\n",
    "        if net_arch == 'minatar':\n",
    "            network = MinAtarQNetwork(**net_kwargs)\n",
    "            init_x = jnp.zeros(obs_shape)\n",
    "        elif net_arch == 'mlp':\n",
    "            network = DenseQNetwork(**net_kwargs)\n",
    "            init_x = jnp.zeros(obs_shape)\n",
    "        else:\n",
    "            raise ValueError(f\"unknown network architecture: {net_arch}\")\n",
    "    \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        params = network.init(_rng, init_x)\n",
    "\n",
    "        \n",
    "        tx = getattr(optax, agent_config.opt)(lrs[net])\n",
    "\n",
    "        train_states.append( TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        )\n",
    "\n",
    "    def params_sum(params):\n",
    "        return sum(jax.tree_util.tree_leaves(jax.tree_map(lambda x: np.prod(x.shape), params)))\n",
    "    print(f\"Total number of params: {params_sum(train_states[0].params) + params_sum(train_states[1].params)}\")\n",
    "    # q_train_state, h_train_state = train_states\n",
    "    grad_h_trace = tree_map(jnp.zeros_like,train_states[0].params)\n",
    "    grad_v_trace = tree_map(jnp.zeros_like,train_states[1].params)\n",
    "    h_trace = 0.0\n",
    "    return AgentState(agent_config, *train_states,h_trace, grad_h_trace, grad_v_trace), rng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_q_trace(e_tmins1,rho_t,gamma,lamda,grad):\n",
    "    # e_{t} = rho_t*gamma*lamda*e_{t-1} + grad)\n",
    "    e_t = tree_map(lambda x,y: (rho_t * gamma * lamda * x) + y, e_tmins1, grad)\n",
    "    return e_t\n",
    "\n",
    "\n",
    "def reset_trace(trace):\n",
    "    return tree.zeros(trace)\n",
    "\n",
    "## Updates for QRC(λ) agent. Equations (26)-(28) in the paper\n",
    "@partial(jax.jit, static_argnames=['terminated', 'truncated', 'is_nongreedy'])\n",
    "def update_step(agent_state, transition, terminated, truncated, is_nongreedy):\n",
    "    obs, action, next_obs, reward = transition\n",
    "\n",
    "    config = agent_state.agent_config\n",
    "    train_state = agent_state.train_state\n",
    "    q_params = train_state.params\n",
    "    h_params = agent_state.h_state.params\n",
    "    h_tm1 = agent_state.h_trace\n",
    "    grad_h_trace_tm1 = agent_state.grad_h_trace\n",
    "    grad_q_trace_tm1 = agent_state.grad_q_trace\n",
    "\n",
    "    def get_q(params):\n",
    "        q = train_state.apply_fn(params, obs)\n",
    "        return q[action]\n",
    "\n",
    "    q_grads = jax.grad(get_q)(q_params)\n",
    "\n",
    "    def get_td_error(params):\n",
    "        q = train_state.apply_fn(params, obs)\n",
    "        q_taken = q[action]\n",
    "        next_q_vect = train_state.apply_fn(params, next_obs)\n",
    "        td_error = (config.gamma * jnp.max(next_q_vect, axis=-1))*(1 - terminated) + reward - q_taken\n",
    "        return td_error\n",
    "        \n",
    "    td_error, td_error_grad = jax.value_and_grad(get_td_error)(q_params)\n",
    "\n",
    "    def get_h(params):\n",
    "        h = agent_state.h_state.apply_fn(params, obs)\n",
    "        return h[action]\n",
    "    h_t, h_grads = jax.value_and_grad(get_h)(h_params)\n",
    "    rho_t = 1.0 # rho here can either be 1 when a greedy action is taken or zero for non greedy action, and we simply just cut the traces if a non-greedy action is taken. i.e, when rho = 0.\n",
    "    h_trace_t = (rho_t * config.gamma * config.lamda * h_tm1) + h_t\n",
    "    grad_h_trace_t = update_q_trace(grad_h_trace_tm1,rho_t, config.gamma, config.lamda, h_grads)\n",
    "    grad_q_trace_t = update_q_trace(grad_q_trace_tm1,rho_t, config.gamma, config.lamda, q_grads)\n",
    "    \n",
    "    # update q\n",
    "    q_update = tree.scale(-h_trace_t, td_error_grad)  # GTD2 update: -trace(h) * ∇δ\n",
    "    if config.gradient_correction:\n",
    "        # TDC update: GTD2 + gradient correction\n",
    "        q_update = tree.add(\n",
    "            tree.scale(td_error, grad_q_trace_t),  # δ * trace(∇q)\n",
    "            tree.scale(-h_t, q_grads),  # -h * ∇q\n",
    "            q_update,\n",
    "        )\n",
    "    q_train_state = train_state.apply_gradients(grads=tree.neg(q_update))  # Flip sign because Flax multiplies by -1\n",
    "    \n",
    "    # update h\n",
    "    delta_z_h = tree.scale(td_error, grad_h_trace_t)\n",
    "    h_h_grad = tree.scale(-h_t, h_grads)\n",
    "    beta_params = tree.scale(-config.reg_coeff, h_params)\n",
    "\n",
    "    h_update = tree_map(lambda x , y , z: -( x + y + z), delta_z_h, h_h_grad, beta_params)\n",
    "    \n",
    "    h_train_state = agent_state.h_state.apply_gradients(grads=h_update)\n",
    "    \n",
    "    if terminated or truncated or is_nongreedy:\n",
    "        h_trace_t = reset_trace(h_trace_t)\n",
    "        grad_h_trace_t = reset_trace(grad_h_trace_t)\n",
    "        grad_q_trace_t = reset_trace(grad_q_trace_t)\n",
    "\n",
    "    return AgentState(config, q_train_state, h_train_state, h_trace_t, grad_h_trace_t, grad_q_trace_t)\n",
    "\n",
    "\n",
    "QRCAgent = Agent(init_agent_state, StreamQAgent.step, update_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1bfea-0723-4678-8591-5eac57d8b7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
